# ğŸš€ Proyecto Integrador â€“ Sistema de AnÃ¡lisis de Ventas
---

# ğŸ§± Proyecto Integrador â€“ Avance 1: Estructura y Carga de Datos

Este repositorio corresponde al **Avance 1** del Proyecto Integrador de Henry. El objetivo de esta fase inicial fue establecer los cimientos de un sistema de anÃ¡lisis de ventas, aplicando conceptos fundamentales de bases de datos, programaciÃ³n orientada a objetos (POO) y buenas prÃ¡cticas de ingenierÃ­a de software.

El trabajo realizado en esta etapa es la base sobre la cual se construirÃ¡n las funcionalidades mÃ¡s complejas en fases posteriores.

---

## âœ… Objetivos Cumplidos en esta Fase

-   **Estructurar el proyecto** con una organizaciÃ³n de carpetas lÃ³gica y escalable.
-   **Modelar las entidades del negocio** (`Product`, `Customer`, etc.) como clases en Python, aplicando principios de POO.
-   **Crear un esquema de base de datos relacional** en PostgreSQL, garantizando la integridad de los datos con claves primarias y forÃ¡neas.
-   **Cargar un conjunto de datos** desde archivos `.csv` a la base de datos de manera eficiente.
-   **Validar la carga de datos** para asegurar que la ingesta fue exitosa.
-   **Implementar pruebas unitarias** simples con `pytest` para verificar la correcta instanciaciÃ³n de los modelos.

---

## ğŸ—‚ï¸ Estructura del Proyecto

Se implementÃ³ una estructura de directorios estÃ¡ndar en la industria para garantizar la **modularidad y la separaciÃ³n de conceptos (Separation of Concerns)**.

```
proyecto_integrador/
â”œâ”€â”€ data/                     # Contiene los archivos CSV de entrada.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1_analisis_exploratorio.ipynb # <-- Notebook inicial para la exploraciÃ³n y validaciÃ³n de datos.
â”‚                   
â”œâ”€â”€ src/                      # CÃ³digo fuente principal de la aplicaciÃ³n.
â”‚   â”œâ”€â”€ __init__.py           # Permite que 'src' sea tratado como un paquete de Python.
â”‚   â””â”€â”€ models.py             # Define las clases POO que representan las tablas.
â”œâ”€â”€ sql/                      # Almacena todos los scripts SQL.
â”‚   â””â”€â”€ load_data.sql         # Script para la creaciÃ³n de tablas y carga de datos.
â”œâ”€â”€ tests/                    # Contiene las pruebas unitarias del proyecto.
â”‚   â””â”€â”€ test_models.py        # Pruebas para las clases definidas en models.py.
â”œâ”€â”€ .env                      # Archivo para almacenar variables de entorno (no versionado).
â”œâ”€â”€ .gitignore                # Especifica los archivos a ignorar por Git.
â”œâ”€â”€ requirements.txt          # Lista de dependencias de Python del proyecto.
â””â”€â”€ README.md                 # Este archivo de documentaciÃ³n.
```

---

## ğŸ›ï¸ Decisiones de DiseÃ±o y Arquitectura (Fase 1)

#### 1. ElecciÃ³n de la Base de Datos: PostgreSQL en Docker
-   **DecisiÃ³n:** Se optÃ³ por **PostgreSQL** por ser una base de datos relacional de cÃ³digo abierto, robusta y muy potente, ideal para anÃ¡lisis de datos.
-   **JustificaciÃ³n:** Utilizar **Docker** para ejecutar la base de datos garantiza un entorno de desarrollo **100% reproducible y aislado**. Cualquier persona que clone el repositorio puede levantar una instancia idÃ©ntica de la base de datos con un solo comando, eliminando problemas de configuraciÃ³n local.

#### 2. Carga de Datos: Comando `COPY`
-   **DecisiÃ³n:** Para cargar los datos desde los archivos `.csv` a las tablas de PostgreSQL, se utilizÃ³ el comando `COPY` nativo de PostgreSQL en lugar de mÃºltiples sentencias `INSERT`.
-   **JustificaciÃ³n:** `COPY` es Ã³rdenes de magnitud **mÃ¡s rÃ¡pido y eficiente** para cargas masivas, ya que estÃ¡ optimizado para leer un flujo de datos directamente desde un archivo. Esto es una prÃ¡ctica estÃ¡ndar en la ingenierÃ­a de datos para la ingesta inicial.

#### 3. Modelado de Datos: ProgramaciÃ³n Orientada a Objetos (POO)
-   **DecisiÃ³n:** Cada tabla del esquema relacional (`countries`, `products`, `sales`, etc.) fue modelada como una clase en Python dentro del archivo `src/models.py`.
-   **JustificaciÃ³n:** Este enfoque permite trabajar con los datos de una manera mÃ¡s intuitiva y alineada con el negocio. En lugar de manejar filas como tuplas o diccionarios genÃ©ricos, operamos con objetos (`Product`, `Customer`) que tienen atributos y mÃ©todos propios. Esto mejora la legibilidad, el mantenimiento y la reutilizaciÃ³n del cÃ³digo. Se implementaron mÃ©todos `__repr__` para facilitar la depuraciÃ³n y mÃ©todos de negocio como `full_name`.

#### 4. Pruebas Unitarias con `pytest`
-   **DecisiÃ³n:** Se implementaron pruebas bÃ¡sicas para validar la correcta creaciÃ³n de los objetos de nuestras clases.
-   **JustificaciÃ³n:** Introducir pruebas desde la primera fase establece una cultura de calidad y robustez. Asegura que los componentes bÃ¡sicos del sistema funcionan como se espera y proporciona una red de seguridad para futuros cambios.

---

## â–¶ï¸ CÃ³mo Ejecutar y Validar el Avance 1

### 1. Prerrequisitos
-   Tener `git`, `Python 3.8+` y `Docker` instalados.

### 2. ConfiguraciÃ³n del Entorno
```bash
# 1. Clona el repositorio
git clone https://github.com/maxifalco18/proyecto_final
cd proyecto_integrador

# 2. Crea y activa un entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# 3. Instala las dependencias
pip install -r requirements.txt
```

### 3. Levantar la Base de Datos y Cargar los Datos
```bash
# 1. Levanta el contenedor de PostgreSQL con Docker
docker run --name pg-integrador -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=admin123 -e POSTGRES_DB=proyecto_integrador -p 5432:5432 -d postgres:15

# 2. Copia los archivos CSV al contenedor
docker cp ./data/. pg-integrador:/data

# 3. Ejecuta el script SQL
# Abre tu cliente de base de datos preferido (DBeaver, DataGrip, etc.),
# conÃ©ctate a la base de datos en Docker y ejecuta el contenido completo
# del archivo 'sql/load_data.sql'.
```

### 4. Correr las Pruebas Unitarias
Para verificar que los modelos POO funcionan correctamente:
```bash
pytest -v tests/test_models.py
```
**Resultado Esperado:** Todas las pruebas deben pasar (`PASSED`), confirmando que los objetos se instancian con los atributos correctos.

---

## ğŸ“Œ DesafÃ­os y Notas Finales

-   **Calidad de Datos:** Durante la carga, se identificaron problemas de formato en las columnas de fecha (`SalesDate`, `ModifyDate`), que contenÃ­an valores invÃ¡lidos.
-   **SoluciÃ³n Temporal:** Para permitir la carga inicial, estas columnas se definieron como de tipo `TEXT` en la base de datos. Se documentÃ³ que este es un punto de "deuda tÃ©cnica" a resolver en fases posteriores mediante un proceso de limpieza de datos (ETL).
-   **Integridad Referencial:** Se establecieron relaciones con claves forÃ¡neas (`FOREIGN KEY`) en el script de creaciÃ³n de tablas para garantizar la consistencia y la integridad de los datos a nivel de base de datos.
-   **ValidaciÃ³n de Carga:** El script `load_data.sql` incluye sentencias `SELECT COUNT(*)` para cada tabla, permitiendo una validaciÃ³n rÃ¡pida y directa de que todos los registros de los CSVs fueron cargados correctamente.

Â¡Por supuesto! AquÃ­ tienes un `README.md` sÃºper completo y descriptivo, enfocado exclusivamente en el **Avance 2**.

Este documento estÃ¡ diseÃ±ado para que un evaluador pueda entender en profundidad las decisiones de arquitectura y diseÃ±o que tomaste en esta fase, demostrando tu capacidad para construir software modular, mantenible y profesional.

---

# ğŸš€ Proyecto Integrador â€“ Avance 2: ModularizaciÃ³n y Patrones de DiseÃ±o

Este documento detalla los avances correspondientes a la **Fase 2** del Proyecto Integrador. El foco de esta etapa fue transformar la base estÃ¡tica del proyecto en una **aplicaciÃ³n Python dinÃ¡mica, modular y robusta**. Se aplicaron patrones de diseÃ±o de software para construir una soluciÃ³n escalable y desacoplada, siguiendo las mejores prÃ¡cticas de la industria.

---

## âœ… Objetivos Cumplidos en esta Fase

-   **Modularizar el sistema** para desacoplar la lÃ³gica de negocio del acceso a los datos.
-   Implementar una **clase de conexiÃ³n** a la base de datos aplicando el patrÃ³n de diseÃ±o **Singleton** para una gestiÃ³n eficiente de recursos.
-   Crear una **capa de acceso a datos** con el patrÃ³n **Repository** para centralizar todas las consultas SQL.
-   Ejecutar consultas desde Python y **formatear los resultados como DataFrames de Pandas** para facilitar el anÃ¡lisis.
-   **Proteger las credenciales** de la base de datos de forma segura utilizando un archivo `.env`.
-   **Integrar la arquitectura completa** en un Jupyter Notebook funcional que demuestre el flujo de datos.
-   AÃ±adir **pruebas unitarias avanzadas** para los patrones implementados, utilizando **mocks** para garantizar pruebas aisladas y rÃ¡pidas.

---

## ğŸ—‚ï¸ Arquitectura de la AplicaciÃ³n (Fase 2)

La estructura del proyecto fue extendida para soportar la nueva lÃ³gica de la aplicaciÃ³n. Los archivos clave de esta fase son:

```
proyecto_integrador/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ notebooks/
|   â”œâ”€â”€ 1_analisis_exploratorio.ipynb # <-- Notebook inicial para la exploraciÃ³n y validaciÃ³n de datos.
â”‚   â””â”€â”€ 2_analisis_de_ventas.ipynb   # <-- NUEVO: Notebook de integraciÃ³n que usa el Repository.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py                # <-- NUEVO: Gestor de conexiÃ³n (Implementa Singleton).
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ repository.py              # <-- NUEVO: Capa de acceso a datos (Implementa Repository).
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ load_data.sql
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_database.py           # <-- NUEVO: Prueba unitaria para el patrÃ³n Singleton.
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_repository.py         # <-- NUEVO: Prueba unitaria para el Repository (con Mocks).
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt               # (Actualizado con sqlalchemy, etc.)
â””â”€â”€ README.md
```

---

## ğŸ›ï¸ Decisiones de IngenierÃ­a y DiseÃ±o (Fase 2)

En esta fase, se tomaron decisiones de arquitectura clave para elevar la calidad del software.

#### 1. PatrÃ³n Singleton para la ConexiÃ³n (`src/database.py`)
-   **Problema a Resolver:** Crear una conexiÃ³n a la base de datos es una operaciÃ³n costosa en tÃ©rminos de tiempo y recursos. Si cada componente de la aplicaciÃ³n (o cada llamada) creara su propia conexiÃ³n, el rendimiento se degradarÃ­a rÃ¡pidamente y se podrÃ­an agotar los recursos del servidor.
-   **SoluciÃ³n Implementada:** Se implementÃ³ el patrÃ³n **Singleton** en la clase `DatabaseConnection`. Este patrÃ³n garantiza que, sin importar cuÃ¡ntas veces se solicite una conexiÃ³n en la aplicaciÃ³n, solo se cree **una Ãºnica instancia** del motor de SQLAlchemy. Este motor gestiona un *pool* de conexiones que se reutilizan de manera eficiente.
-   **JustificaciÃ³n TÃ©cnica:** Esta decisiÃ³n optimiza el rendimiento, reduce la latencia y asegura una gestiÃ³n de recursos predecible y centralizada.

#### 2. PatrÃ³n Repository para el Acceso a Datos (`src/repository.py`)
-   **Problema a Resolver:** Mezclar consultas SQL directamente en la lÃ³gica de negocio (por ejemplo, dentro de un notebook) crea un cÃ³digo fuertemente acoplado. Esto lo hace difÃ­cil de leer, mantener y, sobre todo, probar.
-   **SoluciÃ³n Implementada:** Se creÃ³ la clase `DataRepository`, que actÃºa como una **capa de abstracciÃ³n** entre la aplicaciÃ³n y la base de datos. Esta clase es la Ãºnica responsable de construir y ejecutar consultas SQL. Expone mÃ©todos con nombres claros y relacionados con el negocio (ej. `get_sales_summary_by_country()`).
-   **JustificaciÃ³n TÃ©cnica:** Este patrÃ³n **desacopla** la lÃ³gica de la aplicaciÃ³n de la tecnologÃ­a de la base de datos. Si en el futuro la base de datos cambiara (ej. de PostgreSQL a MySQL) o los datos vinieran de una API, solo tendrÃ­amos que modificar el `DataRepository`. El resto de la aplicaciÃ³n seguirÃ­a funcionando sin cambios, lo que demuestra una arquitectura flexible y mantenible.

#### 3. Pruebas Unitarias con Mocks (`tests/test_repository.py`)
-   **Problema a Resolver:** Las pruebas unitarias deben ser rÃ¡pidas, fiables y no depender de sistemas externos como una base de datos. Probar el `DataRepository` contra una base de datos real serÃ­a una prueba de integraciÃ³n, no unitaria, y serÃ­a lenta y frÃ¡gil.
-   **SoluciÃ³n Implementada:** Se utilizÃ³ la librerÃ­a `unittest.mock` de Python para **simular (mockear)** la conexiÃ³n a la base de datos. Creamos un objeto falso que imita el comportamiento de la conexiÃ³n y le indicamos quÃ© datos falsos debe devolver.
-   **JustificaciÃ³n TÃ©cnica:** Esto nos permite probar la lÃ³gica del `DataRepository` (si construye bien las consultas, si procesa los resultados, etc.) en **total aislamiento**, sin necesidad de tener Docker corriendo o una conexiÃ³n de red activa. Las pruebas son rÃ¡pidas, deterministas y se centran en validar una Ãºnica unidad de cÃ³digo.

---

## â–¶ï¸ CÃ³mo Ejecutar y Validar el Avance 2

Esta guÃ­a asume que la configuraciÃ³n de la **Fase 1** (base de datos en Docker creada y cargada) ha sido completada.

1.  **Actualizar Dependencias:** `pip install -r requirements.txt`
2.  **Configurar `.env`:** Asegurarse de que el archivo `.env` exista en la raÃ­z del proyecto con las credenciales correctas.

#### Ejecutar el AnÃ¡lisis de IntegraciÃ³n:
-   **AcciÃ³n:** Abrir y ejecutar todas las celdas del notebook `notebooks/2_analisis_de_ventas.ipynb`.
-   **Resultado Esperado:** El notebook debe ejecutarse de principio a fin sin errores, mostrando mensajes de inicializaciÃ³n, un **DataFrame de Pandas** con el resumen de ventas y una **visualizaciÃ³n grÃ¡fica** de los resultados. Esto demuestra que todas las capas de la arquitectura se comunican correctamente.

#### Ejecutar las Pruebas Unitarias:
-   **AcciÃ³n:** Correr el siguiente comando en la terminal: `pytest -v tests/`
-   **Resultado Esperado:** Todas las pruebas deben pasar (`PASSED`), confirmando que los patrones Singleton y Repository, asÃ­ como su lÃ³gica interna, funcionan como se esperaba de forma aislada.

---

## âœ”ï¸ Checklist de Requisitos (Fase 2)

| Requisito                            | Estado    | VerificaciÃ³n                                     |
| ------------------------------------ | --------- | ------------------------------------------------ |
| ModularizaciÃ³n y Patrones de DiseÃ±o  | âœ… Cumplido | RevisiÃ³n de `src/database.py` y `src/repository.py`. |
| Clase de conexiÃ³n con Singleton      | âœ… Cumplido | `pytest tests/test_database.py` pasa.            |
| EjecuciÃ³n de consultas desde Python  | âœ… Cumplido | El notebook `2_analisis...` muestra resultados. |
| Resultados como DataFrame de Pandas  | âœ… Cumplido | El notebook muestra un DataFrame.                |
| Notebook de integraciÃ³n funcional    | âœ… Cumplido | EjecuciÃ³n completa del notebook sin errores.     |
| Pruebas unitarias para patrones      | âœ… Cumplido | `pytest tests/test_repository.py` pasa.          |
| Seguridad de credenciales con `.env` | âœ… Cumplido | La conexiÃ³n funciona leyendo desde `.env`.       |

Â¡Absolutamente! AquÃ­ tienes un `README.md` sÃºper completo y descriptivo, enfocado exclusivamente en el **Avance 3**.

Este documento estÃ¡ diseÃ±ado para que un evaluador pueda entender en profundidad las decisiones de ingenierÃ­a, los desafÃ­os y los logros de esta etapa final de optimizaciÃ³n, demostrando tu dominio de tÃ©cnicas avanzadas de SQL y arquitectura de datos.

---

# ğŸš€ Proyecto Integrador â€“ Avance 3: SQL Avanzado y OptimizaciÃ³n

Este documento detalla los avances correspondientes a la **Fase 3 y final** del Proyecto Integrador. El objetivo de esta etapa fue refinar y optimizar el sistema, moviendo la lÃ³gica de anÃ¡lisis compleja desde la aplicaciÃ³n Python hacia la base de datos PostgreSQL.

El principio rector de esta fase fue **"mover el cÃ³mputo a los datos, no los datos al cÃ³mputo"**. Se implementaron tÃ©cnicas avanzadas de SQL para mejorar significativamente el rendimiento, la mantenibilidad y la eficiencia del sistema de anÃ¡lisis.

---

## âœ… Objetivos Cumplidos en esta Fase

-   **Crear y ejecutar consultas SQL analÃ­ticas avanzadas** utilizando **CTEs (Common Table Expressions)** y **Funciones de Ventana** (`ROW_NUMBER()`, `LAG()`).
-   **DiseÃ±ar y crear dos objetos programables en SQL** para encapsular lÃ³gica y simplificar el acceso a los datos. Se implementaron una **Vista** y un **Procedimiento Almacenado**.
-   **Integrar la ejecuciÃ³n** de estas nuevas consultas y objetos desde la capa de datos de Python (`DataRepository`).
-   **Documentar y presentar los resultados** en el Jupyter Notebook final, explicando las tÃ©cnicas utilizadas y los insights obtenidos.
-   **Resolver desafÃ­os de calidad de datos** identificados en fases anteriores, especÃ­ficamente con las columnas de fecha.
-   **Actualizar la documentaciÃ³n final** del proyecto (`README.md`) para reflejar una arquitectura completa y profesional.

---

## ğŸ—‚ï¸ Arquitectura y Ficheros Clave (Fase 3)

La arquitectura de tres capas se mantuvo, pero la **capa de datos (PostgreSQL)** fue significativamente enriquecida.

```
proyecto_integrador/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ products_cleaned.csv       # <-- NUEVO: Archivo generado por el proceso de limpieza.
â”‚   â””â”€â”€ ...
â”œâ”€â”€ notebooks/
|   â”œâ”€â”€ 1_analisis_exploratorio.ipynb # <-- Notebook inicial para la exploraciÃ³n y validaciÃ³n de datos.
â”‚   â””â”€â”€ 2_analisis_de_ventas.ipynb   # <-- NUEVO: Notebook de integraciÃ³n que 
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ repository.py              # (Actualizado con mÃ©todos para llamar a las queries avanzadas).
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ load_data.sql
â”‚   â”œâ”€â”€ 3_advanced_objects.sql     # <-- NUEVO: Script para crear Vistas y Procedimientos Almacenados.
â”‚   â””â”€â”€ 4_update_products_from_cleaned_csv.sql # <-- NUEVO: Script para el experimento de actualizaciÃ³n.
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_database.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_repository.py
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
### JustificaciÃ³n de esta Estructura

-   **`notebooks/`**: La carpeta ahora muestra una progresiÃ³n lÃ³gica.
    -   `1_analisis_exploratorio.ipynb`: Representa el trabajo de la **Fase 1**, donde se realiza un primer contacto con los datos, se validan las cargas y se realizan pruebas iniciales de conexiÃ³n. Es un entregable que demuestra el proceso de descubrimiento.
    -   `2_analisis_de_ventas.ipynb`: Es el **informe final y la demostraciÃ³n principal** del proyecto. No contiene lÃ³gica de conexiÃ³n ni SQL complejo; su Ãºnico propÃ³sito es consumir la aplicaciÃ³n a travÃ©s del `DataRepository` y presentar los anÃ¡lisis avanzados de la **Fase 2 y 3**.
---

## ğŸ›ï¸ Decisiones de IngenierÃ­a y DiseÃ±o (Fase 3)

En esta fase, se delegÃ³ la responsabilidad del anÃ¡lisis complejo a la base de datos, que estÃ¡ diseÃ±ada para manejar estas operaciones de manera Ã³ptima.

#### 1. Consultas AnalÃ­ticas Avanzadas

Para responder preguntas de negocio que van mÃ¡s allÃ¡ de simples agregaciones, se implementaron dos consultas analÃ­ticas clave:

-   **Ranking de Productos por CategorÃ­a:**
    -   **Problema:** Identificar los productos "estrella" dentro de cada categorÃ­a. Un simple `GROUP BY` no puede rankear resultados dentro de un grupo.
    -   **SoluciÃ³n:** Se utilizÃ³ una **CTE** para pre-calcular las ventas por producto y luego la funciÃ³n de ventana **`ROW_NUMBER() OVER (PARTITION BY ...)`** para asignar un ranking a cada producto *dentro* de su categorÃ­a. Esto permite un filtrado trivial de los "Top N" productos.
    -   **JustificaciÃ³n:** Esta tÃ©cnica es estÃ¡ndar para problemas de ranking y es extremadamente eficiente, ya que evita mÃºltiples consultas o un procesamiento complejo en Python.

-   **AnÃ¡lisis de Crecimiento Mensual:**
    -   **Problema:** Calcular la tasa de crecimiento de ingresos mes a mes, lo que requiere comparar la fila de un mes con la del mes anterior.
    -   **SoluciÃ³n:** Se usÃ³ la funciÃ³n de ventana **`LAG()`** para acceder a los ingresos del mes anterior en la misma fila del mes actual, permitiendo el cÃ¡lculo del crecimiento porcentual directamente en la sentencia `SELECT`.
    -   **JustificaciÃ³n:** `LAG()` es la herramienta nativa de SQL para este tipo de anÃ¡lisis de series temporales. Es mucho mÃ¡s performante que traer toda la serie de datos a Pandas para luego hacer un `shift()` y calcular la diferencia.

#### 2. Vista para SimplificaciÃ³n de Datos (`v_ventas_detalladas`)

-   **Problema:** Las consultas de anÃ¡lisis a menudo requerÃ­an unir 5 o 6 tablas (`sales`, `customers`, `products`, etc.), lo que resultaba en cÃ³digo SQL repetitivo, propenso a errores y difÃ­cil de mantener.
-   **SoluciÃ³n:** Se creÃ³ una **VISTA** llamada `v_ventas_detalladas`. Una vista es una tabla virtual almacenada como una consulta `SELECT` que contiene todos los `JOINs` pre-configurados.
-   **JustificaciÃ³n:** La vista **abstrae la complejidad**. Ahora, en lugar de escribir `JOINs` complejos, cualquier consulta puede simplemente hacer un `SELECT` a esta vista como si fuera una tabla normal. Esto reduce errores, asegura consistencia y simplifica radicalmente el desarrollo de nuevas consultas.

#### 3. Procedimiento Almacenado para LÃ³gica de Negocio (`sp_reporte_cliente`)

-   **Problema:** Ciertas operaciones, como generar un reporte completo para un cliente especÃ­fico, representan una lÃ³gica de negocio comÃºn que es reutilizable y requiere un parÃ¡metro de entrada.
-   **SoluciÃ³n:** Se creÃ³ un **PROCEDIMIENTO ALMACENADO** (implementado como una `FUNCTION` en PostgreSQL) que acepta un `CustomerID` como parÃ¡metro. Toda la lÃ³gica para calcular el total gastado, la categorÃ­a favorita y otros KPIs del cliente estÃ¡ encapsulada dentro de este procedimiento en el servidor.
-   **JustificaciÃ³n:**
    -   **Rendimiento:** Reduce el trÃ¡fico de red, ya que la aplicaciÃ³n solo envÃ­a un ID y recibe un pequeÃ±o resultado final.
    -   **Mantenibilidad:** La lÃ³gica de negocio del "reporte de cliente" estÃ¡ en un solo lugar. Si necesita cambiar, se modifica el procedimiento sin tocar la aplicaciÃ³n Python.
    -   **Seguridad y ReutilizaciÃ³n:** Expone una funcionalidad de negocio clara y segura que puede ser utilizada por diferentes partes de la aplicaciÃ³n o incluso por otros sistemas.

---

## ğŸ“Œ DesafÃ­o Clave: Calidad de Datos de Origen

-   **El Problema:** Se confirmÃ³ que las columnas de fecha (`SalesDate`, `ModifyDate`) contenÃ­an datos de texto con formatos invÃ¡lidos (ej. `"31:24.2"`), lo que impedÃ­a su tratamiento como un tipo `TIMESTAMP` nativo.
-   **La SoluciÃ³n Implementada:** Se abordÃ³ este problema de "deuda tÃ©cnica" directamente en la capa de SQL.
    1.  **En la Vista:** Se implementÃ³ una expresiÃ³n `CASE WHEN ...` que utiliza expresiones regulares para validar el formato de la fecha en cada fila. Si el formato es vÃ¡lido, lo convierte a `TIMESTAMP`; si es invÃ¡lido, lo convierte a `NULL`, evitando que las consultas fallen.
    2.  **En el Script de ActualizaciÃ³n (Experimento):** Se diseÃ±Ã³ un script (`4_update_products_from_cleaned_csv.sql`) que utiliza un patrÃ³n de "staging table" para actualizar los datos desde un CSV limpio. Este script contiene la lÃ³gica SQL para parsear y reconstruir las fechas a partir de los deltas de tiempo (ej. `HH:MM.f`), demostrando una soluciÃ³n completa al problema.

---

## â–¶ï¸ CÃ³mo Verificar el Avance 3

1.  **Ejecutar los Objetos SQL:** En un cliente SQL, ejecutar el script `sql/3_advanced_objects.sql` para crear la Vista y el Procedimiento Almacenado.
2.  **Ejecutar el Notebook de AnÃ¡lisis:** Abrir y ejecutar todas las celdas de `notebooks/2_analisis_de_ventas.ipynb`. Las nuevas secciones al final del notebook demostrarÃ¡n:
    -   La ejecuciÃ³n de las consultas de ranking y crecimiento mensual.
    -   La llamada al procedimiento almacenado para generar un reporte de cliente dinÃ¡mico.
    -   El uso implÃ­cito de la vista en las nuevas consultas, que ahora se ejecutan sin errores de fecha.

---

## âœ”ï¸ Checklist de Requisitos (Fase 3)

| Requisito                               | Estado    | VerificaciÃ³n                                     |
| --------------------------------------- | --------- | ------------------------------------------------ |
| Consultas con CTEs y Funciones Ventana  | âœ… Cumplido | MÃ©todos en `repository.py` y resultados en notebook. |
| CreaciÃ³n de Vista                       | âœ… Cumplido | `sql/3_advanced_objects.sql` y su uso en consultas. |
| CreaciÃ³n de Procedimiento Almacenado    | âœ… Cumplido | `sql/3_advanced_objects.sql` y su uso desde Python. |
| IntegraciÃ³n de todo desde Python        | âœ… Cumplido | El notebook se ejecuta y llama a todos los mÃ©todos. |
| DocumentaciÃ³n en Notebook               | âœ… Cumplido | Celdas de Markdown explican cada paso del anÃ¡lisis. |
| DocumentaciÃ³n Final en `README.md`      | âœ… Cumplido | Este mismo documento.                            |